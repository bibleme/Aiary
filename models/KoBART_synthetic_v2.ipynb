{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bibleme/Aiary/blob/one_line_diary/KoBART_synthetic_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15200f5c",
        "outputId": "77496d99-8b44-41cd-d70f-3fad576bfc91"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 먼저 실행: NumPy 버전을 1.x대로 강제 고정\n",
        "!pip install \"numpy<2.0\"\n",
        "\n",
        "# 2. 그 다음 나머지 라이브러리 설치\n",
        "!pip install -q \"transformers>=4.46.0,<5\" sentencepiece accelerate datasets evaluate rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lceK33yRU_n9",
        "outputId": "7ff41af3-05bb-4069-b289-e7eafa0494bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 한 줄 일기 → 하루일기 KoBART 파인튜닝\n",
        "# ================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \"transformers>=4.46.0,<5\" sentencepiece accelerate datasets evaluate rouge-score\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    BartForConditionalGeneration,\n",
        "    PreTrainedTokenizerFast,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------------\n",
        "# 경로 및 상수 설정\n",
        "# --------------------------\n",
        "BASE_DIR = \"/content/drive/MyDrive/aiary\"\n",
        "\n",
        "MERGED_CSV = os.path.join(\n",
        "    BASE_DIR,\n",
        "    \"data/parenting_dataset_v4/final_merged_dataset.csv\"\n",
        ")\n",
        "\n",
        "OUTPUT_DIR = os.path.join(\n",
        "    BASE_DIR,\n",
        "    \"models/day_diary_from_summary_v2\"\n",
        ")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "BASE_MODEL_NAME = \"gogamza/kobart-base-v2\"  # 쓰는 KoBART 모델명으로 바꿔도 됨\n",
        "MAX_INPUT_LEN = 256    # summary 길이 그렇게 안 길어서 256 정도\n",
        "MAX_TARGET_LEN = 384   # 하루일기 300~400자 정도 가정\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# --------------------------\n",
        "# 1) 데이터 로드\n",
        "#    merged.csv: id, source, diary, summary\n",
        "# --------------------------\n",
        "assert os.path.exists(MERGED_CSV), f\"MERGED_CSV 파일 없음: {MERGED_CSV}\"\n",
        "df = pd.read_csv(MERGED_CSV)\n",
        "\n",
        "required_cols = {\"id\", \"source\", \"diary\", \"summary\"}\n",
        "assert required_cols.issubset(df.columns), f\"CSV 컬럼 부족: {df.columns}\"\n",
        "\n",
        "df[\"diary\"] = df[\"diary\"].astype(str).str.strip()\n",
        "df[\"summary\"] = df[\"summary\"].astype(str).str.strip()\n",
        "df = df[(df[\"diary\"] != \"\") & (df[\"summary\"] != \"\")].reset_index(drop=True)\n",
        "\n",
        "print(f\"전체 샘플 수: {len(df)}\")\n",
        "print(df[\"source\"].value_counts())\n",
        "\n",
        "# --------------------------\n",
        "# 2) train / val / test 분할\n",
        "#    - val/test는 real만\n",
        "# --------------------------\n",
        "df_real = df[df[\"source\"] == \"real\"].reset_index(drop=True)\n",
        "df_synth = df[df[\"source\"] == \"synthetic\"].reset_index(drop=True)\n",
        "\n",
        "print(f\"real 샘플 수: {len(df_real)}\")\n",
        "print(f\"synthetic 샘플 수: {len(df_synth)}\")\n",
        "\n",
        "real_train, real_temp = train_test_split(\n",
        "    df_real,\n",
        "    test_size=0.3,\n",
        "    random_state=RANDOM_SEED,\n",
        "    shuffle=True,\n",
        ")\n",
        "real_val, real_test = train_test_split(\n",
        "    real_temp,\n",
        "    test_size=0.5,\n",
        "    random_state=RANDOM_SEED,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "train_df = pd.concat([real_train, df_synth], ignore_index=True)\n",
        "train_df = train_df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "val_df = real_val.reset_index(drop=True)\n",
        "test_df = real_test.reset_index(drop=True)\n",
        "\n",
        "print(\"\\n분할 결과\")\n",
        "print(f\"Train: {len(train_df)} (real={len(real_train)}, synthetic={len(df_synth)})\")\n",
        "print(f\"Val:   {len(val_df)} (real only)\")\n",
        "print(f\"Test:  {len(test_df)} (real only)\")\n",
        "\n",
        "# --------------------------\n",
        "# 3) Dataset 정의\n",
        "# --------------------------\n",
        "class SummaryToDiaryDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_input_len=256, max_target_len=384):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_target_len = max_target_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        summary = str(row[\"summary\"])\n",
        "        diary = str(row[\"diary\"])\n",
        "\n",
        "        # 인풋: 한 줄 일기 세트\n",
        "        input_text = f\"[SUMMARY]\\n{summary}\\n[DIARY]\"\n",
        "        target_text = diary\n",
        "\n",
        "        model_inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_input_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            labels = self.tokenizer(\n",
        "                target_text,\n",
        "                max_length=self.max_target_len,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        input_ids = model_inputs[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = model_inputs[\"attention_mask\"].squeeze(0)\n",
        "        labels_ids = labels[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        labels_ids[labels_ids == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels_ids,\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nv6etwALqUt",
        "outputId": "d86bbfa4-2a51-4a74-85d7-fff17913b684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "전체 샘플 수: 512\n",
            "source\n",
            "synthetic    300\n",
            "real         212\n",
            "Name: count, dtype: int64\n",
            "real 샘플 수: 212\n",
            "synthetic 샘플 수: 300\n",
            "\n",
            "분할 결과\n",
            "Train: 448 (real=148, synthetic=300)\n",
            "Val:   32 (real only)\n",
            "Test:  32 (real only)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 4) 토크나이저 / 모델 로드\n",
        "# --------------------------\n",
        "print(\"\\n[INFO] 토크나이저 / 모델 로드 중...\")\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(BASE_MODEL_NAME)\n",
        "model = BartForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"device = {device}\")\n",
        "\n",
        "# --------------------------\n",
        "# 5) Dataset, Collator, Trainer\n",
        "# --------------------------\n",
        "train_dataset = SummaryToDiaryDataset(train_df, tokenizer, MAX_INPUT_LEN, MAX_TARGET_LEN)\n",
        "val_dataset = SummaryToDiaryDataset(val_df, tokenizer, MAX_INPUT_LEN, MAX_TARGET_LEN)\n",
        "test_dataset = SummaryToDiaryDataset(test_df, tokenizer, MAX_INPUT_LEN, MAX_TARGET_LEN)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=5,          # 너무 오래 돌리면 오버피팅, 일단 5epoch\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=20,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=[],\n",
        "    seed=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 6) 학습\n",
        "# --------------------------\n",
        "print(\"\\n[INFO] 학습 시작...\")\n",
        "train_result = trainer.train()\n",
        "trainer.save_state()\n",
        "\n",
        "# --------------------------\n",
        "# 7) 모델 / 토크나이저 저장\n",
        "# --------------------------\n",
        "print(\"\\n[INFO] 모델 저장 중...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"train_result.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_result.metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --------------------------\n",
        "# 8) 학습 곡선 시각화\n",
        "# --------------------------\n",
        "log_history = trainer.state.log_history\n",
        "train_steps, train_losses = [], []\n",
        "eval_steps, eval_losses = [], []\n",
        "\n",
        "for log in log_history:\n",
        "    if \"loss\" in log and \"step\" in log:\n",
        "        train_steps.append(log[\"step\"])\n",
        "        train_losses.append(log[\"loss\"])\n",
        "    if \"eval_loss\" in log and \"step\" in log:\n",
        "        eval_steps.append(log[\"step\"])\n",
        "        eval_losses.append(log[\"eval_loss\"])\n",
        "\n",
        "if train_steps:\n",
        "    plt.figure()\n",
        "    plt.plot(train_steps, train_losses, label=\"train_loss\")\n",
        "    if eval_steps:\n",
        "        plt.plot(eval_steps, eval_losses, label=\"eval_loss\")\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.title(\"Training & Validation Loss (Summary→Diary)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    curve_path = os.path.join(OUTPUT_DIR, \"training_curves.png\")\n",
        "    plt.savefig(curve_path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"[INFO] 학습 곡선 저장: {curve_path}\")\n",
        "else:\n",
        "    print(\"[WARN] train loss 로그가 없습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "5sgNFpBka2Zb",
        "outputId": "40f6cc45-3c3b-4bad-bd1c-512384b39898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] 토크나이저 / 모델 로드 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device = cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4020037289.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] 학습 시작...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [560/560 04:57, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.225800</td>\n",
              "      <td>2.856390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.831500</td>\n",
              "      <td>2.791125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.510200</td>\n",
              "      <td>2.821344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.207500</td>\n",
              "      <td>2.866042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.056700</td>\n",
              "      <td>2.897255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] 모델 저장 중...\n",
            "[INFO] 학습 곡선 저장: /content/drive/MyDrive/aiary/models/day_diary_from_summary_v2/training_curves.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 9) [수정됨] 한 샘플에 대해 하루일기 생성 함수 (KoBART 전용)\n",
        "# --------------------------\n",
        "def generate_diary_from_summary(summary_text: str, max_len: int = MAX_TARGET_LEN) -> str:\n",
        "    input_text = f\"[SUMMARY]\\n{summary_text}\\n[DIARY]\"\n",
        "    enc = tokenizer(\n",
        "        input_text,\n",
        "        max_length=MAX_INPUT_LEN,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # 필요한 애들만 꺼내서 device로 올리기\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=256,\n",
        "            min_length=50,\n",
        "            repetition_penalty=2.5,\n",
        "            no_repeat_ngram_size=3,\n",
        "            do_sample=True,\n",
        "            temperature=0.5,\n",
        "            top_p=0.85,\n",
        "            early_stopping=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred = pred.replace(\"[DIARY]\", \"\").strip()\n",
        "    return pred\n",
        "\n",
        "# --------------------------\n",
        "# 10) 테스트 셋 예시 3개 출력\n",
        "# --------------------------\n",
        "print(\"\\n[INFO] 테스트 셋 예시 3개 출력 (summary → diary)\")\n",
        "\n",
        "for i in range(min(3, len(test_df))):\n",
        "    sample = test_df.iloc[i]\n",
        "    summary_text = sample[\"summary\"]\n",
        "    diary_gold = sample[\"diary\"]\n",
        "\n",
        "    pred = generate_diary_from_summary(summary_text)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"[Sample {i+1}]\")\n",
        "    print(\"[SUMMARY (INPUT)]\")\n",
        "    print(summary_text)\n",
        "    print(\"\\n[GROUND TRUTH DIARY]\")\n",
        "    print(diary_gold)\n",
        "    print(\"\\n[PREDICTED DIARY]\")\n",
        "    print(pred)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# --------------------------\n",
        "# 11) 테스트셋 전체 평가지표 (ROUGE_L만 간단히)\n",
        "#      - 생성은 다양성이 커서 ROUGE는 참고용 정도로만\n",
        "# --------------------------\n",
        "\n",
        "# --------------------------\n",
        "# 11) 테스트셋 Perplexity 계산\n",
        "# --------------------------\n",
        "print(\"\\n[INFO] 테스트셋 Perplexity 계산 중...\")\n",
        "\n",
        "test_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_eval_batch_size=4,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# loss-only evaluation\n",
        "test_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=test_args,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "test_metrics = test_trainer.evaluate()\n",
        "\n",
        "test_loss = test_metrics[\"eval_loss\"]\n",
        "test_ppl = torch.exp(torch.tensor(test_loss)).item()\n",
        "\n",
        "print(f\"[TEST LOSS] {test_loss:.4f}\")\n",
        "print(f\"[TEST PPL]  {test_ppl:.4f}\")\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"test_perplexity.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(\n",
        "        {\"test_loss\": float(test_loss), \"test_ppl\": float(test_ppl)},\n",
        "        f,\n",
        "        ensure_ascii=False,\n",
        "        indent=2,\n",
        "    )\n",
        "\n",
        "print(\"\\n[INFO] 테스트셋 전체 ROUGE_L 계산 중...\")\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "all_preds = []\n",
        "all_refs = []\n",
        "\n",
        "for i in range(len(test_df)):\n",
        "    sample = test_df.iloc[i]\n",
        "    summary_text = sample[\"summary\"]\n",
        "    diary_gold = sample[\"diary\"]\n",
        "\n",
        "    pred = generate_diary_from_summary(summary_text)\n",
        "    all_preds.append(pred)\n",
        "    all_refs.append(diary_gold)\n",
        "\n",
        "rouge_result = rouge.compute(\n",
        "    predictions=all_preds,\n",
        "    references=all_refs,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print(\"\\n[TEST METRICS] (참고용)\")\n",
        "print(f\"ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
        "\n",
        "metrics_path = os.path.join(OUTPUT_DIR, \"test_metrics_rougeL.json\")\n",
        "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(\n",
        "        {\"rougeL\": rouge_result[\"rougeL\"]},\n",
        "        f,\n",
        "        ensure_ascii=False,\n",
        "        indent=2,\n",
        "    )\n",
        "\n",
        "print(f\"\\n[INFO] 테스트 평가지표 저장: {metrics_path}\")\n",
        "print(\"\\n[INFO] 전체 파이프라인 완료\")\n",
        "print(f\"모델/토크나이저/그래프 저장 위치: {OUTPUT_DIR}\")\n",
        "\n",
        "# --------------------------\n",
        "# 12) 테스트셋 전체 생성 결과 저장 (CSV + JSONL)\n",
        "# --------------------------\n",
        "print(\"\\n[INFO] 테스트셋 전체 생성 결과 저장 중...\")\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for i in range(len(test_df)):\n",
        "    row = test_df.iloc[i]\n",
        "    summary_text = row[\"summary\"]\n",
        "    diary_gold = row[\"diary\"]\n",
        "\n",
        "    pred = generate_diary_from_summary(summary_text)\n",
        "\n",
        "    test_results.append({\n",
        "        \"id\": row[\"id\"],\n",
        "        \"summary\": summary_text,\n",
        "        \"gold_diary\": diary_gold,\n",
        "        \"predicted_diary\": pred\n",
        "    })\n",
        "\n",
        "# CSV 저장\n",
        "results_csv_path = os.path.join(OUTPUT_DIR, \"test_generated_results.csv\")\n",
        "pd.DataFrame(test_results).to_csv(results_csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# JSONL 저장\n",
        "results_jsonl_path = os.path.join(OUTPUT_DIR, \"test_generated_results.jsonl\")\n",
        "with open(results_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in test_results:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"[INFO] CSV 저장 완료: {results_csv_path}\")\n",
        "print(f\"[INFO] JSONL 저장 완료: {results_jsonl_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YvujNy4_hcux",
        "outputId": "2422fe2d-8932-484a-ca86-0e18c93dbb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] 테스트 셋 예시 3개 출력 (summary → diary)\n",
            "================================================================================\n",
            "[Sample 1]\n",
            "[SUMMARY (INPUT)]\n",
            "1. 딸이 방 안에서 불만스러운 표정을 짓고 있다. 2. 엄마가 청소한 방이 정돈되어 있지만, 딸의 마음은 복잡해 보인다. 3. 두 사람의 대화가 따뜻한 분위기를 만들어낸다.\n",
            "\n",
            "[GROUND TRUTH DIARY]\n",
            "오늘 딸이 나한테 실망했다고 하더라. 방 청소를 해줬는데도 불만이 많아서 좀 속상했어. 그래도 딸이 나를 바라보는 눈빛이 귀여워서 화가 나기도 했지만, 결국은 이해해주기로 했어.\n",
            "\n",
            "[PREDICTED DIARY]\n",
            "오늘 딸이 방이 더러워서 화가 나서 그랬어. 그래서 내가 잘못한 게 없나 싶더라. 딸의 마음이 조금은 이해되지만, 그래도 이렇게 대화가 잘 되는 날이 언제 있을까 싶어! 앞으로는 더 많은 대화를 나누고 소통하는 시간을 가져야겠다. 딸을 위해서라도 이런 작은 일들이 소중하게 느껴져, 정말 행복해 보여서 좋네. 앞으로도 계속 사랑스러운 시간이 많으면 좋겠어!\" 하고 다짐했지. 이 순간을 잊지 말자며 오늘도 최선을 다해야겠다는 생각이 들어.\n",
            "딸에게 항상 고맙다고 말해주고 싶었거든? 그런 말을 듣고 나니 내 마음도 따뜻해졌고, 나도 다시 힘을 내게 됐어.\n",
            "\n",
            "내가 뭘 어떻게 해야 할지 고민이 많아졌는데, 그 마음을 이해할 수 있었네요. 요즘 애가 많이 성장해서 엄마를 닮았구나 하는 걸 느꼈어요. 아들이 자라나는 모습을 보면서 힘이 나는 것 같아요... \n",
            "주변에서 도와주는 것도 중요하지만, 서로 믿고 의지할 때 더욱 큰 행복을 느끼게 돼서, 하루종일 행복한 시간이었음 해줘라. 다음에는 좀 특별하고 따뜻한 마음으로 함께 할게 있어 줘 감사하야. 그렇게 소중한 기억을 간직했으면 한 번쯤 남길래야 한다. 매일매일 너와 함께하는 특별한 날들을 만들어가고 있음이 너무 뿌듯하다. 가족과 지인들과 소소한 일상 속에서 느끼는 기쁨을 잊어버릴\n",
            "================================================================================\n",
            "================================================================================\n",
            "[Sample 2]\n",
            "[SUMMARY (INPUT)]\n",
            "1. 아들이 아침 식탁에 앉아 있지만, 음식에 손을 대지 않고 있다.\n",
            "2. 차려진 아침이 고요한 식탁 위에 놓여 있다.\n",
            "3. 아들의 표정이 고민스러워 보이며, 아침을 먹지 않으려는 모습이 느껴진다.\n",
            "\n",
            "[GROUND TRUTH DIARY]\n",
            "요즘 아들이 아침을 잘 안 먹어서 걱정이네. 아침을 차려 줘도 그냥 지나치고 나가버려. 왜 그런지 물어봐야겠다. 아들이 아침을 먹어야 건강한데, 이렇게 안 먹으면 안 되잖아. 다음엔 꼭 물어보고, 아침을 맛있게 먹을 수 있도록 도와줘야겠어.\n",
            "\n",
            "[PREDICTED DIARY]\n",
            "오늘 아들이 아침 식탁에 앉아 있었는데, 음식물이 고요한 것 같아서 마음이 아팠어. 아침에 일어나서 먹으려고 해도 너무 지쳐서, 밥도 안 먹고 잠만 자고 있었지. 그런 모습이 참 슬프고, 미안해. 그래도 이렇게 자라는 모습을 보니, 다시 힘을 내야겠다. 내일은 더 잘 챙겨야겠다는 생각이 들어.\n",
            "\n",
            "아침이 정말 소중하다는 걸 새삼 느꼈네. 아들과의 대화 속에서 이런 작은 행복들이 쌓여가는 게 느껴져 기분이 좋았어! 오늘도 건강하게 자라길 바라며, 하루를 마무리할 수 있도록 노력해야 할 시간이야. 아들의 말대로 내가 매일매일 최선을 다하는 모습 보니까 뿌듯하더라구요. 그래서 오늘은 꼭 맛있는 음식을 차려놓았거든요”라고 말해주고 싶었어.\n",
            "그렇게 열심히 하는 아들을 보며 나도 힘이 나더라고 생각했지.\n",
            "오늘 아이가 먹는 건 항상 소중한 시간이라 생각해보려고 해봤는데, 그게 얼마나 중요한지 깨달아가고 있어라. 이 순간을 잊지 말고 계속 응원해주면 좋겠다 싶어-\"\n",
            "[정기수요일] 어제보다 조금 나아진 모습으로 돌아오기를 바래봅니다. 가족과 함께 시간을 보낼 때지만, 그 행복한 기억이 내 마음을 따뜻히 해주었으면 좋겠다고 다짐하고 있네!\n",
            "자랑스러운 우리 아기야, 이제 또 다른 행복을 만들어갈게 될 거야. 언제까지나\n",
            "================================================================================\n",
            "================================================================================\n",
            "[Sample 3]\n",
            "[SUMMARY (INPUT)]\n",
            "1. 아기가 병원 침대에서 조용히 누워 있다. 2. 작은 손가락이 가끔씩 움직이며 꿈꾸는 듯하다. 3. 병실의 조명이 부드럽게 아이의 얼굴을 감싸고 있다.\n",
            "\n",
            "[GROUND TRUTH DIARY]\n",
            "오늘 아이가 조산으로 태어난 걸 생각하니 마음이 아프네. 내가 몸 관리를 잘 못한 것 같아서 미안한 마음이 들어. 그래도 아이가 건강하게 퇴원하면 정말 잘 키울 거야. 아이를 위해 정성을 다할 거고, 건강하게 자라도록 열심히 노력할 거야. 응원해줘서 고마워.\n",
            "\n",
            "[PREDICTED DIARY]\n",
            "오늘 아기가 병원 침대에서 조용히 누워 있었어. 작은 손가락이 가끔씩 움직이며 꿈꾸는 듯하더라. 그 모습이 너무 귀여워서 나도 모르게 미소가 지어졌네. 아기와의 소중한 시간들이 이렇게 소중하게 느껴져서, 정말 감사해! 앞으로도 이런 순간들을 많이 만들어보고 싶어요”라고 말했지. 이 아이가 건강해지기를 바래보며, 더 많은 사랑을 주고 싶다. \n",
            "\n",
            "아이가 병원에서 하루 종일 병실에서 기다리다가 결국 세상을 등지는데, 어떻게 해야 할지 모르겠어? 오늘도 힘든 육아 속에서도 아이와 함께하는 시간이 참 행복하다고 생각하니 마음이 따뜻해졌다 다.\n",
            "내가 낳은 예쁜 딸, 내 곁에 있어줘서 고맙다고 말해주고 싶었어.\n",
            "오늘은 엄마랑 함께 있는 시간을 가졌을 때야. 사랑스러운 아들이 태어났으면 좋겠고, 매일매일 행복한 마음으로 살 수 있길 바라면서 다시 한 번 다짐하고 있다 고 말하고 싶다.\n",
            "주변에 응원하는 사람들이 많으니 언제까지나 계속 지켜볼게요. 사랑이 넘치는 세상이었음을 잊지 않을 거야. 내일도 또 다른 행복을 주는 날이 될 것 같아 기대돼.\n",
            "그렇게 소소한 일상이 우리를 더욱 특별해지는 기분으로 만들었거든.\" \"엄마에게 잘 키워달라고 기도해야겠다.\" 그런 마음을 전할 때, 내가 얼마나 큰 힘이 되는지를 새삼 느꼈고 기억될 거라 믿\n",
            "================================================================================\n",
            "\n",
            "[INFO] 테스트셋 Perplexity 계산 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1394716629.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  test_trainer = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST LOSS] 2.5922\n",
            "[TEST PPL]  13.3585\n",
            "\n",
            "[INFO] 테스트셋 전체 ROUGE_L 계산 중...\n",
            "\n",
            "[TEST METRICS] (참고용)\n",
            "ROUGE-L: 0.0000\n",
            "\n",
            "[INFO] 테스트 평가지표 저장: /content/drive/MyDrive/aiary/models/day_diary_from_summary_v2/test_metrics_rougeL.json\n",
            "\n",
            "[INFO] 전체 파이프라인 완료\n",
            "모델/토크나이저/그래프 저장 위치: /content/drive/MyDrive/aiary/models/day_diary_from_summary_v2\n",
            "\n",
            "[INFO] 테스트셋 전체 생성 결과 저장 중...\n",
            "[INFO] CSV 저장 완료: /content/drive/MyDrive/aiary/models/day_diary_from_summary_v2/test_generated_results.csv\n",
            "[INFO] JSONL 저장 완료: /content/drive/MyDrive/aiary/models/day_diary_from_summary_v2/test_generated_results.jsonl\n"
          ]
        }
      ]
    }
  ]
}