{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ7UhM9DbTZGkGlv9XSSgC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bibleme/Aiary/blob/one_line_diary/%ED%95%98%EB%A3%A8%EC%9D%BC%EA%B8%B0_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCW5y68g77VZ",
        "outputId": "379a1eab-917f-4285-f610-023e07361d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] ëª¨ë¸ / í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\n",
            "[INFO] device = cpu\n",
            "[INFO] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
            "[INFO] íŒŒì¼ì—ì„œ ì½ì€ ë¬¸ì¥ ê°œìˆ˜: 5\n",
            "[LOG] ì›ë³¸ í•œ ì¤„ ì¼ê¸°ë“¤:\n",
            "- ë”°ëœ»í•œ ì°¨ ì•ˆì—ì„œ ê³¼ì ë¨¹ë°© ì‹œì‘ğŸªğŸš—ğŸ’•\n",
            "- ê³ ê¸° ëƒ„ìƒˆ ê°€ë“í•œ ë”°ëœ»í•œ ê°€ì¡± ì™¸ì‹ì‹œê°„ğŸ–ğŸ¥°\n",
            "- ì¥ë‚œê° ì´ê³¼ ì¹œêµ¬, ì˜¤ëŠ˜ë„ ì‹ ë‚˜ê²Œ ë†€ì•˜ì–´ìš”ğŸ”«ğŸ¤ âœ¨\n",
            "- ë”°ëˆí•œ ê³ ê¸° ëƒ„ìƒˆì— ê¸°ëŒ€ ê°€ë“í•œ ë°¤ğŸ–âœ¨\n",
            "- ë¶ˆê½ƒ ì†ì—ì„œ ì•„ë¹  ì† ì¡ê³  ë”°ëœ»í•¨ì„ ëŠê¼ˆì–´ìš”ğŸ”¥ğŸ¤—\n",
            "\n",
            "[LOG] ëª¨ë¸ì— ë“¤ì–´ê°ˆ SUMMARY í¬ë§·:\n",
            "1. ë”°ëœ»í•œ ì°¨ ì•ˆì—ì„œ ê³¼ì ë¨¹ë°© ì‹œì‘\n",
            "2. ê³ ê¸° ëƒ„ìƒˆ ê°€ë“í•œ ë”°ëœ»í•œ ê°€ì¡± ì™¸ì‹ì‹œê°„\n",
            "3. ì¥ë‚œê° ì´ê³¼ ì¹œêµ¬, ì˜¤ëŠ˜ë„ ì‹ ë‚˜ê²Œ ë†€ì•˜ì–´ìš”\n",
            "4. ë”°ëˆí•œ ê³ ê¸° ëƒ„ìƒˆì— ê¸°ëŒ€ ê°€ë“í•œ ë°¤\n",
            "5. ë¶ˆê½ƒ ì†ì—ì„œ ì•„ë¹  ì† ì¡ê³  ë”°ëœ»í•¨ì„ ëŠê¼ˆì–´ìš”\n",
            "\n",
            "[INFO] í•˜ë£¨ ì¼ê¸° ìƒì„± ì¤‘...\n",
            "\n",
            "[GENERATED DIARY]\n",
            "ì˜¤ëŠ˜ ì•„ì¹¨ ì°¨ ì•ˆì—ì„œ ê³¼ì ë¨¹ë°© í–ˆì–´. ê³ ê¸°ì™€ í–¥ì´ ì§„í•˜ê²Œ í¼ì§€ë©´ì„œ, ë‚˜ë„ ë©ë‹¬ì•„ ê¸°ë¶„ì´ ì¢‹ì•˜ë„¤. ì´ë ‡ê²Œ ë”°ëœ»í•œ ë‚ ì´ ì–¸ì œê¹Œì§€ë‚˜ ê³„ì†ë˜ë©´ ì¢‹ê² ì–´! ì˜¤ëŠ˜ë„ ì•„ë¹  ì† ì¡ê³  ë”°ëœ»í•´ì¡Œê±°ë“ . ë¶ˆê½ƒë†€ì´ í•˜ëŠ” ëª¨ìŠµì„ ë³´ë©° ë‹¤ì‹œ í˜ì„ ë‚´ì„œ í•¨ê»˜ ë†€ì•„ë³´ë ¤ê³  í•´.\n",
            "\n",
            "ì˜¤ëŠ˜ì€ ì •ë§ í–‰ë³µí–ˆë„¤!\n",
            "ë”°ëœ»í•œ í•˜ë£¨ì˜€ëŠ”ë°, ì´ì œ ì´ ìˆœê°„ì„ ì˜ì›íˆ ê°„ì§í•˜ê³  ì‹¶ì–´ì ¸ìš” \"êº…!\" í•˜ë©° ì‹ ë‚˜ê²Œ ë›°ì–´ë‹¤ë…”ì–´.\n",
            "ë°¤ì— ì°½ë°–ìœ¼ë¡œ í–‡ì‚´ì´ ë“¤ì–´ì™€ì„œ, ì•„ëŠ‘í•¨ì´ ë”í•´ì ¸ ë”ìš± í‰í™”ë¡œìš´ ì‹œê°„ì´ì—ˆì§€. ê·¸ ì‘ì€ ì†ìœ¼ë¡œ ê³µì›ì—ì„œ í‚¤ìš´ ê½ƒë“¤ì´ ë¹›ë‚˜ê³  ìˆì—ˆìœ¼ë‹ˆ ë„ˆë¬´ ê¸°ë»¤ê³ , ì•ìœ¼ë¡œë„ ì´ëŸ° ì†Œì¤‘í•œ ì‹œê°„ì´ ë§ì•˜ìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì–´ìš”. \n",
            "ê½ƒìì²˜ëŸ¼ ë¹›ë‚˜ëŠ” ë°¤ì— ë„ˆì™€ í•¨ê»˜í•˜ëŠ” ì‹œê°„ì„ ì†Œì¤‘íˆ ë§Œë“¤ì–´ì£¼ê¸¸ ë°”ë¼ë©°, ì˜¤ëŠ˜ì€ ê¼­ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì„ ë¬¼ì„ ì¤€ë¹„í• ê²Œìš”.\n",
            "ìœ¡ì•„ëŠ” í˜ë“¤ì§€ë§Œ, ì‚¬ë‘ì´ ê°€ë“í•œ ë‚ ë“¤ì„ ê¸°ì–µí•´ë‘ê³  ì‹¶ì–´ì„œ ë§ˆìŒì´ ë‹¤ì°¨ë¶„í•´ì§€ë”ë¼êµ¬ìš”\n",
            "\"\n",
            "ë˜ í•œ ë²ˆ ë„ì „í•˜ëŠ” ê²Œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ëŠ” ê±¸ ëŠê¼ˆì–´, ìœ¡ì•„ì˜ ì–´ë ¤ì›€ ì†ì—ì„œë„ ì‚¬ë‘ì„ ëŠë¼ëŠ” ê²ƒ ê°™ì•„ ë§¤ì¼ë§¤ì¼ ê°ì‚¬í•´ì•¼ê² ë‹¤. ê°€ì¡±ë“¤ê³¼\n",
            "\n",
            "[INFO] JSON ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/aiary/outputs/generated_diary_from_one_line.json\n",
            "[INFO] TXT ì €ì¥ ì™„ë£Œ:  /content/drive/MyDrive/aiary/outputs/generated_diary_from_one_line.txt\n",
            "\n",
            "[INFO] ì¸í¼ëŸ°ìŠ¤ ì „ì²´ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# AIARY Inference (v2)\n",
        "# íŒŒì¼ ë‚´ ì—¬ëŸ¬ \"í•œ ì¤„ ì¼ê¸°\" ì „ì²´ë¥¼ í•©ì³ì„œ\n",
        "# í•˜ë£¨ ì¼ê¸° 1ê°œ ìƒì„±í•˜ëŠ” ë²„ì „\n",
        "# ============================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \"transformers>=4.36.0,<5\" sentencepiece accelerate emoji regex pandas\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import regex as re\n",
        "import emoji\n",
        "import pandas as pd\n",
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "\n",
        "# ---------------------------------------\n",
        "# 1) ê²½ë¡œ ì„¤ì •\n",
        "# ---------------------------------------\n",
        "BASE_DIR = \"/content/drive/MyDrive/aiary\"\n",
        "\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"models/day_diary_from_summary_v2\")\n",
        "ONE_LINE_FILE = os.path.join(BASE_DIR, \"outputs/one_line_texts.txt\")\n",
        "\n",
        "OUTPUT_JSON = os.path.join(BASE_DIR, \"outputs/generated_diary_from_one_line.json\")\n",
        "OUTPUT_TXT  = os.path.join(BASE_DIR, \"outputs/generated_diary_from_one_line.txt\")\n",
        "\n",
        "MAX_INPUT_LEN = 256\n",
        "MAX_TARGET_LEN = 220\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 2) ì´ëª¨ì§€ ì œê±° + í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
        "# ---------------------------------------\n",
        "def remove_emoji(text: str) -> str:\n",
        "    text = emoji.replace_emoji(text, \"\")\n",
        "    emoji_pattern = re.compile(r\"[\\p{Emoji}\\p{Emoji_Presentation}\\p{Extended_Pictographic}]\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(\"\", text)\n",
        "\n",
        "def clean_sentence(text: str) -> str:\n",
        "    text = str(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 3) ëª¨ë¸ / í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "# ---------------------------------------\n",
        "print(\"[INFO] ëª¨ë¸ / í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    raise FileNotFoundError(f\"ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_DIR}\")\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_DIR)\n",
        "model = BartForConditionalGeneration.from_pretrained(MODEL_DIR)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"[INFO] device = {device}\")\n",
        "print(\"[INFO] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 4) ìš”ì•½(ì—¬ëŸ¬ ì¤„) â†’ í•˜ë£¨ì¼ê¸° ìƒì„± í•¨ìˆ˜\n",
        "# ---------------------------------------\n",
        "def generate_diary_from_summary(summary_text: str,\n",
        "                                max_len: int = MAX_TARGET_LEN) -> str:\n",
        "    \"\"\"\n",
        "    summary_text: \"1. ~\\n2. ~\\n3. ~\"ì²˜ëŸ¼ ì—¬ëŸ¬ ì¤„ë¡œ ëœ ìš”ì•½ ì „ì²´\n",
        "    return: KoBARTê°€ ìƒì„±í•œ í•˜ë£¨ ì¼ê¸° í…ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "\n",
        "    input_text = f\"[SUMMARY]\\n{summary_text}\\n[DIARY]\"\n",
        "\n",
        "    enc = tokenizer(\n",
        "        input_text,\n",
        "        max_length=MAX_INPUT_LEN,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_len,\n",
        "            min_length=40,\n",
        "            no_repeat_ngram_size=3,\n",
        "            repetition_penalty=2.0,\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9,\n",
        "            early_stopping=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred = pred.replace(\"[DIARY]\", \"\").strip()\n",
        "    return pred\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 5) one_line_texts.txt ë¡œë“œ (ì „ì²´ê°€ í•˜ë£¨ ì¸í’‹)\n",
        "# ---------------------------------------\n",
        "if not os.path.exists(ONE_LINE_FILE):\n",
        "    raise FileNotFoundError(f\"í•œ ì¤„ ì¼ê¸° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ONE_LINE_FILE}\")\n",
        "\n",
        "with open(ONE_LINE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "if not raw_lines:\n",
        "    raise ValueError(\"one_line_texts.txt ì•ˆì— ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(f\"[INFO] íŒŒì¼ì—ì„œ ì½ì€ ë¬¸ì¥ ê°œìˆ˜: {len(raw_lines)}\")\n",
        "print(\"[LOG] ì›ë³¸ í•œ ì¤„ ì¼ê¸°ë“¤:\")\n",
        "for ex in raw_lines:\n",
        "    print(\"-\", ex)\n",
        "\n",
        "# ì´ëª¨ì§€ ì œê±° + ê³µë°± ì •ë¦¬\n",
        "cleaned_sentences = [clean_sentence(s) for s in raw_lines]\n",
        "\n",
        "# \"1. ~\" í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ ë¶™ì—¬ì„œ summary êµ¬ì„±\n",
        "bullet_lines = []\n",
        "for i, sent in enumerate(cleaned_sentences, start=1):\n",
        "    # ì´ë¯¸ '1.' ê°™ì´ ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ë©´ ê·¸ëŒ€ë¡œ ë‘˜ ìˆ˜ë„ ìˆì§€ë§Œ,\n",
        "    # ì—¬ê¸°ì„œëŠ” ë¬´ì¡°ê±´ ìƒˆë¡œ ë²ˆí˜¸ ë¶™ì´ëŠ” ê²Œ ê¹”ë”í•¨\n",
        "    bullet_lines.append(f\"{i}. {sent}\")\n",
        "\n",
        "combined_summary = \"\\n\".join(bullet_lines)\n",
        "\n",
        "print(\"\\n[LOG] ëª¨ë¸ì— ë“¤ì–´ê°ˆ SUMMARY í¬ë§·:\")\n",
        "print(combined_summary)\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6) í•˜ë£¨ì¼ê¸° ìƒì„±\n",
        "# ---------------------------------------\n",
        "print(\"\\n[INFO] í•˜ë£¨ ì¼ê¸° ìƒì„± ì¤‘...\")\n",
        "generated_diary = generate_diary_from_summary(combined_summary)\n",
        "print(\"\\n[GENERATED DIARY]\")\n",
        "print(generated_diary)\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 7) ê²°ê³¼ ì €ì¥ (JSON + TXT)\n",
        "# ---------------------------------------\n",
        "result_obj = {\n",
        "    \"raw_lines\": raw_lines,                # ì›ë³¸ í•œ ì¤„ ì¼ê¸°ë“¤ (ì´ëª¨ì§€ í¬í•¨)\n",
        "    \"cleaned_summary_bullets\": bullet_lines,  # \"1. ~\" í˜•ì‹ summary\n",
        "    \"combined_summary\": combined_summary,\n",
        "    \"generated_diary\": generated_diary,\n",
        "}\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(result_obj, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(generated_diary + \"\\n\")\n",
        "\n",
        "print(f\"\\n[INFO] JSON ì €ì¥ ì™„ë£Œ: {OUTPUT_JSON}\")\n",
        "print(f\"[INFO] TXT ì €ì¥ ì™„ë£Œ:  {OUTPUT_TXT}\")\n",
        "print(\"\\n[INFO] ì¸í¼ëŸ°ìŠ¤ ì „ì²´ ì™„ë£Œ\")\n"
      ]
    }
  ]
}